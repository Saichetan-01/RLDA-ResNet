# ğŸš€ RLDA-ResNet: CNN Acceleration using Lookup Tables

This project implements **RLDA (Residual Lookup-based Dot-product Approximation)** on **ResNet-18** to reduce runtime **Multiplyâ€“Accumulate (MAC)** operations in convolutional neural networks while maintaining competitive accuracy on the **CIFAR-10** dataset.

Instead of performing expensive multiplications during inference, the model uses **precomputed lookup tables (LUTs)** and **nearest-centroid search**, making it suitable for **hardware-efficient and low-power systems**.

---

## ğŸ“Œ Project Highlights

- âœ… Baseline **ResNet-18** trained on CIFAR-10  
- âœ… LUT generation using **K-Means clustering (K = 32)**  
- âœ… Replacement of `Conv2d` with **RLDAConv** (lookup + add operations)  
- âœ… Fine-tuning RLDA-ResNet to recover accuracy  
- âœ… Evaluation using **Accuracy, Precision, Recall, F1-Score**  
- âœ… Confusion Matrix with **TP, TN, FP, FN**  

---

## ğŸ§  Core Idea

### Original ResNet
```

Convolution = weight Ã— input  â†’ millions of MAC operations

```

### RLDA-ResNet
```

input patch â†’ nearest centroid â†’ LUT lookup â†’ addition

````

âœ” Multiplications are **precomputed offline**  
âœ” Inference uses **lookup + add**, not multiply  
âœ” Trade-off: small accuracy drop for large compute savings  

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ rlda_conv.py         # RLDAConv: LUT-based convolution replacing Conv2d
â”‚   â”œâ”€â”€ rlda_resnet.py       # RLDA-ResNet18 architecture (paper-based implementation)
â”‚   â””â”€â”€ original_resnet.py   # Baseline ResNet-18 for accuracy comparison
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ luts.py              # K-Means clustering and LUT (codebook) generation
â”‚   â”œâ”€â”€ inspect_model.py     # Utility to inspect model layers, weights, and shapes
â”‚   â””â”€â”€ check_gpu.py         # GPU / device environment verification
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_resnet.py      # Train baseline ResNet-18 on CIFAR-10
â”‚   â”œâ”€â”€ train_rlda.py        # Train / fine-tune RLDA-ResNet model
â”‚   â””â”€â”€ confusion_matrix.py # Evaluation, metrics, and visualization
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cifar-10-batches-py/ # CIFAR-10 dataset
â”‚
â”œâ”€â”€ trained_resnet18_cifar10.pth  # Baseline trained ResNet weights
â”œâ”€â”€ best_rlda_resnet18.pth        # Best RLDA-ResNet checkpoint
â”œâ”€â”€ lut_layer*_conv*.pth          # Generated LUT files (K = 32 centroids)
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # Project documentation
````

---

## ğŸ§ª Dataset

**CIFAR-10**

* 60,000 images (32Ã—32 RGB)
* 10 classes
* 50,000 training images
* 10,000 testing images

---

## âš™ï¸ Requirements

* Python â‰¥ 3.9
* PyTorch
* torchvision
* numpy
* matplotlib
* seaborn
* scikit-learn

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Run (Step-by-Step)

### 1ï¸âƒ£ Train Baseline ResNet-18

```bash
python scripts/train_resnet.py
```

Output:

```
trained_resnet18_cifar10.pth
```

---

### 2ï¸âƒ£ Generate Lookup Tables (LUTs)

```bash
python utils/luts.py
```

Output:

```
lut_layer*_conv*.pth   (K = 32 centroids per convolution layer)
```

---

### 3ï¸âƒ£ Train / Fine-Tune RLDA-ResNet

```bash
python scripts/train_rlda.py
```

Output:

```
best_rlda_resnet18.pth
```

---

### 4ï¸âƒ£ Evaluate Model & Plot Confusion Matrix

```bash
python scripts/confusion_matrix.py
```

Outputs:

* Confusion Matrix
* Overall Accuracy
* Precision, Recall, F1-Score
* TP, TN, FP, FN (per class)

---

## ğŸ“Š Metrics Used

From the confusion matrix, the following metrics are computed:

* **Accuracy**
* **Precision**
* **Recall**
* **F1-Score**
* **True Positive (TP)**
* **True Negative (TN)**
* **False Positive (FP)**
* **False Negative (FN)**

Metrics are reported **per class and overall**.

---

## ğŸ”¬ Technical Details

* **Clustering Method:** K-Means
* **Number of Centroids (K):** 32
* **Nearest Search:** 1-nearest centroid (not KNN classifier)
* **Distance Metric:** L1 distance (`torch.cdist`)
* **RLDAConv Parameters:**

  * centroids
  * dot_centroids
  * residual_centroids
* **Training:**

  * Optimizer: SGD
  * Loss: CrossEntropyLoss
  * Fine-tuning after approximation

---

## ğŸ“‰ Accuracy Trade-off

| Model              | Accuracy       | Computation             |
| ------------------ | -------------- | ----------------------- |
| Original ResNet-18 | Higher         | Full MAC operations     |
| RLDA-ResNet-18     | Slightly Lower | LUT-based (MAC-reduced) |

The accuracy drop is **expected** due to approximation and is a known trade-off in hardware-aware deep learning.

---

## ğŸ“ Academic Relevance

This project demonstrates:

* CNN acceleration techniques
* Approximate computing
* Lookup-table-based inference
* Accuracy vs efficiency trade-offs

Suitable for:

* Major Project
* Research-oriented coursework
* Hardware-aware machine learning exploration

---

## ğŸ”® Future Improvements

* Increase K (e.g., 64 centroids)
* Apply RLDA only to deeper layers
* Quantized LUTs
* FPGA / Edge deployment
* Benchmark MAC reduction and latency

---

## ğŸ‘¨â€ğŸ’» Author

**Sai Chetan Barathala**
Electronics & Communication Engineering
Major Project â€“ Deep Learning & CNN Acceleration



